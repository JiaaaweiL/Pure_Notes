# 集合通信简介：
  - 集合通信算法
  - 集合通信链路
  - 集合通信操作
  - 集合通信域管理

## 为什么需要XCCL？
当网络太大的时候，单卡不够用，必须引入分布式训练：   
四种分布式训练方法：   
- Data Parallelism 算出梯度后，用CCL库进行梯度聚合，需要all reduced
- Pipeline Parallelism 梯度回传，也需要ALL reduced
- Tensor Parallelism  参数切分，需要ALL GATHER tensor 合并
- Expert Parallelism 上述三种的组合，啥都需要
![image](https://github.com/user-attachments/assets/269a545c-0e35-49e6-96f2-6ff43f64ae49)
(!!! 看大模型分布式训练补充完整！！！)

## XCCL 基础架构
计算神经网络的过程中，每一层神经网络都会计算出一个梯度GRAD， 如果反向传播得到一个梯度马上就用ALL Reduce进行梯度规约，再集群中将计算与通信同步串行，那么集群利用路就会很差。
因此，计算通信解耦。然后用并行计算和通信（overlap）
- 提升集群训练性能（模型利用路MFU）
- 避免计算假死锁（计算耗时长，通信长时间等待）
![image](https://github.com/user-attachments/assets/114f3c64-8f2c-4435-99e0-62b737bd8146)
CCL很重要，因为和系统的拓扑/链路硬件强相关

